{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to Least Square Error (LSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Least Square Error (LSE)?\n",
    "\n",
    "LSE is defined as the sum of squared residuals of a dataset. To put this in perspective, let's take\n",
    "$x^{(i)}$ and $y^{(i)}$ an input vector and label respectively, and our prediction: $y_{pred}^{(i)}$ after running our model on the input $x^{(i)}$\n",
    "\n",
    "Then we have a residual (or error) of a specific example as $(y^{i} - y_{pred}^{i})$ which is the difference between the actual output and our predicted output.\n",
    "\n",
    "Now we need to square the residuals. $(y^{i} - y_{pred}^{i})^{2}$\n",
    "There are several reasons we do this, but I will mention two here:\n",
    "1. If the difference between actual and predicted is negative i.e. $(y^{i} - y_{pred}^{i}) < 0 $ then by squaring the term, we always receive a positive outcome, so the differences are treated simply by their magnitude rather than their magnitude and sign\n",
    "\n",
    "1. By squaring the term, larger differences are valued *MORE* than smaller differences since squaring a term is non-linear\n",
    "\n",
    "Finally, we need to sum over each data point (example) to obtain the total error over all data points:\n",
    "$$\\\\ LSE = \\sum_{i}(y^{i} - y_{pred}^{i})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the LSE Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the directory containing your code file to sys.path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "from models.LinearRegression import LinearRegression\n",
    "\n",
    "# CREATE SOME DUMMY DATA\n",
    "x = np.linspace(0, 10, 10)\n",
    "y = x + 2 + np.random.rand() - 0.5 # y is a linear function of x with some noise\n",
    "\n",
    "# reshape the data to place into the model\n",
    "X = np.reshape(x, (x.shape[0], 1))\n",
    "\n",
    "# initialize our model (I will use my Linear Regression here)\n",
    "model = LinearRegression(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding this formula is not very tricky. We first need to calculuate $y_{pred}^{i}$. This is done so by our model, (likely Linear Regression). To do so we execute the following for a vector of inputs $x^{(i)}$. We can think of this as a matrix $X$ where the rows represent each example, and the columns are each feature of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7789378 , 1.05777379, 1.33660978, 1.61544576, 1.89428175,\n",
       "       2.17311774, 2.45195373, 2.73078972, 3.00962571, 3.28846169])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this function is a vector $y_{pred}$ where each entry is the $y_{pred}^{(i)}$ for the corresponding $x^{(i)}$\n",
    "<br> We now need to take our actual outputs $y$ and element-wise subtract the $y_{pred}$ vector. \n",
    "<br> We can do this easily if both y and X are numpy arrays.\n",
    "<br>Then we need to square each element and sum all of the elements together. (Since the subtraction of each output by the prediction results in a vector of residuals) \n",
    "<br>Thus the entire function is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSELoss(y_pred, y):\n",
    "    return np.sum((model(X) - y) ** 2)\n",
    "\n",
    "LSELoss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Least Squares? (Digging into the theory)\n",
    "##### *(Skip to the end for a high-level summary without all of the math)*\n",
    "Assume we have the same circumstances as above with $y_{pred}^{(i)}, y^{(i)}, x^{(i)}$\n",
    "\n",
    "Now we can relate our predicted output and actual output as the following:\n",
    "$\\\\ y^{(i)} = y_{pred}^{(i)} + \\epsilon^{(i)}$, where $\\epsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^{2})$\n",
    "This means that our predicted output + some error (unmodelled effects, noise, etc.) is equal to our actual output,\n",
    "for each example. We are assuming that our unmodelled error is normally distributed, and that each $\\epsilon^{i}$ is *i.i.d. (Independent and Identically Distributed)*\n",
    "\n",
    "By assuming this, we now have a probability model of our unmodelled error:\n",
    "$$ P(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{\\epsilon^{(i)^{2}}}{2\\sigma^{2}}]$$\n",
    "\n",
    "The probability of this specific unmodelled error occuring (since we are assuming is a normal r.v.) is the same as the probability of our output occuring, given our input and using the formula relating our output to predicted output, we can replace our unmodelled error:\n",
    "\n",
    "$$ P(y^{(i)} | x^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(y^{(i)} - y_{pred}^{(i)})^{2}}{2\\sigma^{2}}] \\implies  y^{(i)} | x^{(i)} \\sim \\mathcal{N}(y_{pred}^{(i)}, \\sigma^{2})$$\n",
    "\n",
    "So now we know that our output follows a normal distribution with an expectation of our predicted output, meaning that \n",
    "we can find a likelihood function of our predicted output throughout all $m$ examples. Since $y_{pred}^{(i)}$ is a function of our parameters we define our Likelihood function as a function of our parameters, rather than $y_{pred}^{(i)}$. Let $$g^{(i)}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} \\\\ \\theta \\rightarrow y_{pred}^{(i)}$$\n",
    "\n",
    "This is just so that our predicted output is defined as a function of our parameter vector $\\theta$\n",
    "<br> Then we have the Likelihood as:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\prod_{i = 1}^{m} P(y^{(i)} | x^{(i)}) $$\n",
    "\n",
    "By taking the log of the likelihood, we then get a summation:\n",
    "\n",
    "$$ \\log{\\mathcal{L}(\\theta)} = l(\\theta) = \\sum_{i=1}^{m}\\log{\\frac{1}{\\sqrt{2 \\pi \\sigma}}} + \\log{exp[-\\frac{(y^{(i)} - g^{(i)}(\\theta))^{2}}{2\\sigma^{2}}]}\n",
    "\\\\ = m\\log{\\frac{1}{\\sqrt{2 \\pi \\sigma}}} + \\sum_{i=1}^{m}-\\frac{(y^{(i)} - g^{(i)}(\\theta))^{2}}{2\\sigma^{2}}$$\n",
    "\n",
    "Now by using Maximum Likelihood Estimation we need to choose the parameters $\\theta$ of our model to maximize $l(\\theta)$\n",
    "\n",
    "$$ \\underset{\\theta}{\\mathrm{argmax }} [m\\log{\\frac{1}{\\sqrt{2 \\pi \\sigma}}} + \\sum_{i=1}^{m}-\\frac{(y^{(i)} - g^{(i)}(\\theta))^{2}}{2\\sigma^{2}}]\n",
    "\\\\ = \\underset{\\theta}{\\mathrm{argmax}} [\\sum_{i=1}^{m}-\\frac{(y^{(i)} - g^{(i)}(\\theta))^{2}}{2\\sigma^{2}}]\n",
    "\\\\ = \\underset{\\theta}{\\mathrm{argmin}} [\\sum_{i=1}^{m}(y^{(i)} - g^{(i)}(\\theta))^{2}]\n",
    "\\\\ = \\underset{\\theta}{\\mathrm{argmin}} [LSE]\n",
    "$$\n",
    "\n",
    "Ok so, at a high level, if we assume that our outputs follow a normal distribution, centered around our predicted output (our predicted output should be the average of the real outputs), then by using maximum likelihood estimation, we end up minimizing the least sqaures error to find our optimal parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

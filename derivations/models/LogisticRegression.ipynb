{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Logistic Regression?\n",
    "\n",
    "Logistic regression is a binary classification algorithm, that converts a feature vector $x$ into a probability distribution over $2$ classes.\n",
    "\n",
    "The idea is that we give the model a $x \\in \\mathbb{R}^{n}$ and we want it to output the probability of obtaining a certain class. We usually denote the classes 0 and 1 respecitively, and the model outputs the probability of obtaining a 1 given the input features. We denote our hypothesis function as:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1 + exp[\\theta^Tx + b]} $$\n",
    "\n",
    "This is also known as the sigmoid function and we can denote this as:\n",
    "$$ h_{\\theta}(x) = \\sigma(\\theta^Tx + b) $$\n",
    "\n",
    "You may notice that we are still using a linear model where we multiply weights $\\theta$ with our feature vector and add a bias $b$. The difference is that after performing this operation we place it into a sigmoid function which effectively squashes the line into a range of $(0, 1)$. This means that we have converted from a range of $(-\\infty, \\infty)$ to $(0, 1)$, which allows us to interpret the output in a different way. This concept is the main idea behind Generalized Linear Models which both Logistic and Linear regression are a part of. The function that converts the output into a specific probability distribution is the inverse of what is known as the 'canonical link function'. In the case of logistic regression the canonical link function is known as the logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I have mentioned previously, we wish to make this an accurate model to represent the probability of a certain outcome happening given some inputs. In order to do so, we need to optimize our parameters to some data.\n",
    "\n",
    "$$\\text{Let } X = (x^{(1)}, x^{(2)}, \\rightarrow x^{(m)})^{T} \\in \\mathbb{R}^{m \\times n}, x^{(i)} \\in \\mathbb{R}^{n} $$\n",
    "$$ y = (y^{(1)}, y^{(2)}, ..., y^{(m)}) \\in \\mathbb{R}^{m}, y^{(i)} \\in \\mathbb{R}$$\n",
    "\n",
    "In other words: $X$ is a matrix where each column represents a feature, and each row represents a training example, and $y$ is a vector of each training example output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A common loss function to optimize over for Logistic Regression is the Binary Cross Entropy Loss.\n",
    "If you want to know more about how this loss is derived and why we use it for Logistic Regression, check out the \n",
    "CrossEntropyLoss.ipynb notebook for more info. We will define our loss function as a function of our parameters\n",
    "$J(\\theta, b) = BCE$\n",
    "\n",
    "$$ \\implies J(\\theta, b) = -\\sum_{i = 1}^{m}y^{(i)} \\cdot \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\cdot \\log(1 - h_{\\theta}(x^{(i)})) $$\n",
    "$$ = -\\sum_{i = 1}^{m}y^{(i)} \\cdot \\log(\\sigma(\\theta^Tx^{(i)} + b)) + (1 - y^{(i)}) \\cdot \\log(1 - \\sigma(\\theta^Tx^{(i)} + b)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Basically, we get a very large negative number when we predict very incorrectly and a very small negative number when\n",
    "we predict close to correctly. This results in a very large loss if there were a lot of very large discrepencies between\n",
    "our predictions and the actual output or a smaller loss when we predicted close to correct most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize our parameters we need to find the parameters that minimize this function. We need to find:\n",
    "\n",
    "$$ \\underset{\\theta, b}{\\mathrm{argmin}} [J(\\theta, b)]$$\n",
    "\n",
    "To do so we may use an optimization algorithm such as gradient descent. This means we need to compute the gradient of the loss function w.r.t. the parameters of the model. The gradient of the loss is defined as:\n",
    "\n",
    "$$  \\nabla_{J}(\\theta) = - \\frac{\\partial h}{\\partial \\theta}^T y_{h} $$\n",
    "$$ \\text{Where } \\; \\frac{\\partial h}{\\partial \\theta} = (\\frac{\\partial h}{\\partial \\theta_{1}} \\rightarrow \\frac{\\partial h}{\\partial \\theta_{n}}, \\frac{\\partial h}{\\partial b}) \\in \\mathbb{R}^{m \\times n + 1},$$\n",
    "$$ \\frac{\\partial h}{\\partial \\theta_{j}} = (\\frac{\\partial h_{\\theta}(x^{(1)})}{\\partial \\theta_{j}}, ..., \\frac{\\partial h_{\\theta}(x^{(m)})}{\\partial \\theta_{j}}) \\in \\mathbb{R}^{m},$$\n",
    "$$ \\frac{\\partial h}{\\partial b} = (\\frac{\\partial h_{\\theta}(x^{(1)})}{\\partial b}, ..., \\frac{\\partial h_{\\theta}(x^{(m)})}{\\partial b}) \\in \\mathbb{R}^{m}$$\n",
    "$$ \\text{And } \\; y_g = (y - h_{\\theta}(x)) \\otimes (h_{\\theta}(x) - h_{\\theta}(x) \\odot h_{\\theta}(x))  $$\n",
    "\n",
    "Where $\\otimes$ is the Hadamard division (element-wise division) and $\\odot$ is the Hadamard product (element-wise product)\n",
    "$\\frac{\\partial g}{\\partial \\theta}$ is known as the Jacobian of $h$\n",
    "<br><br> *If you are interested in how this gradient was computed, check out the CrossEntropyLoss.ipynb notebook where I go over it in detail*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we only have to compute $\\frac{\\partial g}{\\partial \\theta}$ and let the loss function compute the rest and then matrix by vector multiply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing $\\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_j}$ for some arbitrary $i \\in \\{1,...,m\\}, j \\in \\{1,...,n\\}$\n",
    "\n",
    "$$ h_{\\theta}(x^{(i)}) = \\sigma(\\theta^Tx^{(i)} + b) $$\n",
    "\n",
    "The derivative of $\\theta^Tx^{(i)}$ w.r.t $\\theta_j$ is just $x^{(i)}_j$, and the derivative of $\\sigma(a)$ is $\\sigma(a)(1-\\sigma(a))$. Thus by chain rule:\n",
    "\n",
    "$$ \\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_j} =  \\sigma(\\theta^Tx^{(i)} + b) \\cdot (1 - \\sigma(\\theta^Tx^{(i)} + b)) \\cdot x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the derivative w.r.t $b$ which by chain rule is the same, except that the derivative of $\\theta^Tx^{(i)}$ w.r.t $b$ is just $1$. So we have:\n",
    "\n",
    "$$ \\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial b} =  \\sigma(\\theta^Tx^{(i)} + b) \\cdot (1 - \\sigma(\\theta^Tx^{(i)} + b))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally, we have our Jacobian as:\n",
    "\n",
    "$$\\frac{\\partial h}{\\partial \\theta} = \n",
    "\\begin{bmatrix} \n",
    "    h_{\\theta}(x^{(1)})\\cdot (1 - h_{\\theta}(x^{(1)})) \\cdot x^{(1)}_1 & \\dots & h_{\\theta}(x^{(1)})\\cdot (1 - h_{\\theta}(x^{(1)})) \\cdot x^{(1)}_n & h_{\\theta}(x^{(1)})\\cdot (1 - h_{\\theta}(x^{(1)})) \\\\\n",
    "    \\vdots & \\ddots  & \\vdots & \\vdots \\\\\n",
    "    h_{\\theta}(x^{(m)})\\cdot (1 - h_{\\theta}(x^{(m)})) \\cdot x^{(m)}_1 & \\dots & h_{\\theta}(x^{(m)})\\cdot (1 - h_{\\theta}(x^{(m)})) \\cdot x^{(m)}_n & h_{\\theta}(x^{(m)})\\cdot (1 - h_{\\theta}(x^{(m)}))\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose this matrix further to make calculations easier.\n",
    "\n",
    "$$ \\text{Let } \\; \\partial\\sigma_h = (h_{\\theta}(x^{(1)})\\cdot (1 - h_{\\theta}(x^{(1)})), ..., h_{\\theta}(x^{(m)})\\cdot (1 - h_{\\theta}(x^{(m)})))) \\in \\mathbb{R}^{m} $$\n",
    "Then if we brodcast $\\partial\\sigma_h$ to a matrix,\n",
    "\n",
    "$$ H = (\\leftarrow \\partial\\sigma_h \\rightarrow) \\in \\mathbb{R}^{m \\times n + 1} $$\n",
    "\n",
    "Then our Jacobian matrix is:\n",
    "$$\\frac{\\partial h}{\\partial \\theta} = [X \\quad b_m] \\odot H, \\quad b = (1, 1, ..., 1) \\in \\mathbb{R}^{m} $$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$  \\nabla_{J}(\\theta) = ([X \\quad b_m] \\odot H)^T (-y_g) $$\n",
    "\n",
    "We can now convert our broadcasted matrix back into a vector, since we can see that when multiplying $-y_g$ by a matrix it will effectively be broadcasted before summed so we can use this to our advantage.\n",
    "\n",
    "Our final gradient is:\n",
    "\n",
    "$$  \\nabla_{J}(\\theta) = [X \\quad b_m]^T (\\partial\\sigma_h \\odot (-y_g)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough this may look daunting, remember we only have to worry about the components that we calculated from our Jacobian. This means we do not have to worry about the $-y_g$ term as that will be provided by our loss function. And we can see that $X$ is provided to us and $b_m$ is just a vector of 1. So the only computation we really have to worry about is $\\partial \\sigma_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the directory containing your code file to sys.path\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "from loss.CrossEntropyLoss import BinaryCrossEntropyLoss\n",
    "from optimizers.SGDOptimizer import SGDOptimizer\n",
    "\n",
    "# CREATE SOME DUMMY DATA\n",
    "x = np.linspace(0, 10, 20)\n",
    "y = np.array([1 if el + 2*np.random.rand() else 0 for el in x])\n",
    "\n",
    "# reshape the data to place into the model\n",
    "X = np.reshape(x, (x.shape[0], 1))\n",
    "\n",
    "# initialize our loss (I will use Binary Cross Entropy Loss here) and our optimizer\n",
    "loss = BinaryCrossEntropyLoss()\n",
    "optimizer = SGDOptimizer(0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we need to initialize the weights of the model. We need to specify the feature space. In this example I will use a dimension of 1, however this scales to $n$ dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.32456918], Bias: [0.36886216]\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.rand(1) # shape is (1,)\n",
    "bias = np.random.rand(1) # shape is (1,)\n",
    "print(f\"Weights: {weights}, Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the forward pass of our model, i.e the hypothesis function, and run through it for all training examples to get a vector of predictions. We first must calculate the linear component $\\theta^Tx$ and then calculate the sigmoid of that linear component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59118401, 0.63173985, 0.67051471, 0.70709979, 0.74119056,\n",
       "       0.77258958, 0.80120098, 0.82701863, 0.85011066, 0.87060235,\n",
       "       0.88865949, 0.90447329, 0.91824767, 0.93018916, 0.9404994 ,\n",
       "       0.94936981, 0.95697831, 0.96348741, 0.96904355, 0.97377722])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(X):\n",
    "    linear = np.dot(X, weights) + bias # linear\n",
    "    return 1 / (1 + np.exp(-linear)) # sigmoid\n",
    "\n",
    "y_pred = forward(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's specify our gradient function. This should take in the calculated gradients for the loss function and we will multiply with the derivatives of our model (Using the formula we derived above)\n",
    "\n",
    "We start by running a forward pass to obtain our $h_{\\theta}(x)$. Then using this, we know we need the vector $\\partial \\sigma_h$ which is just a vector of $h_{\\theta}(x) \\odot (1 - h_{\\theta}(x)) = h_{\\theta}(x) - h_{\\theta}(x) \\odot h_{\\theta}(x)$ We then multiply this by the $-y_g$ which is supplied via our loss function. \n",
    "Finally we take the dot with our input matrix X, and concatenate on taking the dot with a vector of $1s$ which is equivalent to just summing up the entries of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.87121986, -3.14202359])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grads(X, loss_grad):\n",
    "        y_pred = forward(X) # we need to run a forward pass again, since we use this in our Jacobian\n",
    "        adjusted_loss_grads = loss_grad * (y_pred - y_pred**2) # this line is equivalent to our vector of partials of the sigmoids * (-yg)\n",
    "        return np.concatenate((np.dot(X.T, adjusted_loss_grads), [np.sum(adjusted_loss_grads)]))\n",
    "\n",
    "loss_grad = loss.grads(y_pred, y)\n",
    "model_grads = grads(X, loss_grad)\n",
    "model_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to apply them using the optimizer to get the updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.41328138], Bias: [0.40028239]\n"
     ]
    }
   ],
   "source": [
    "params = np.concatenate((weights, bias))\n",
    "optimizer.step(params, model_grads)\n",
    "weights = np.array(params[:weights.size])\n",
    "bias = np.array(params[weights.size:])\n",
    "print(f\"Weights: {weights}, Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would repeat this until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You may extend the concept of linear regression into multiple classes. This is known as Softmax Regression. The same concepts apply, however our hypothesis function now outputs a probability distribution (vector) over our $k$ classes. Each of the entries in the vector gets it's own linear model with it's own parameters to optimize. However after evaluating each of these linear models, the entries are placed into what is known as a softmax function. $$S(x^i) = \\frac{\\exp[x^i]}{\\sum_{j = 1}^{k}\\exp[x^j]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k = 2, this boils down to Logistic Regression, and this can be proved, thus it is an extension of Logistic Regression into multiple classes. Your loss function is also likely to change to Categorical Cross Entropy Loss which works on multi class problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Logistic Regression?\n",
    "\n",
    "Logistic regression is a binary classification algorithm, that converts a feature vector $x$ into a probability distribution over $2$ classes.\n",
    "\n",
    "The idea is that we give the model a $x \\in \\mathbb{R}^{n}$ and we want it to output the probability of obtaining a certain class. We usually denote the classes 0 and 1 respecitively, and the model outputs the probability of obtaining a 1 given the input features. We denote our hypothesis function as:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1 + exp[\\theta^Tx + b]} $$\n",
    "\n",
    "This is also known as the sigmoid function and we can denote this as:\n",
    "$$ h_{\\theta}(x) = \\sigma(\\theta^Tx + b) $$\n",
    "\n",
    "You may notice that we are still using a linear model where we multiply weights $\\theta$ with our feature vector and add a bias $b$. The difference is that after performing this operation we place it into a sigmoid function which effectively squashes the line into a range of $(0, 1)$. This means that we have converted from a range of $(-\\infty, \\infty)$ to $(0, 1)$, which allows us to interpret the output in a different way. This concept is the main idea behind Generalized Linear Models which both Logistic and Linear regression are a part of. The function that converts the output into a specific probability distribution is the inverse of what is known as the 'canonical link function'. In the case of logistic regression the canonical link function is known as the logit function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

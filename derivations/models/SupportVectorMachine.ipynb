{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to the Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of Support Vector Machine?\n",
    "\n",
    "A support vector machine (SVM) is natively a binary classification supervised machine learning method. On it's own, a support vector machine is not particularly powerful, however when paired with what is known as the kernel trick, it becomes a very powerful tool for classifying data that is not obviously linearly seperable.\n",
    "\n",
    "Due to the nature of this pairing, I will first go over the kernel trick, and then after the SVM and how (and why) we can apply the kernel trick to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is a very useful tool that we can use in a variety of ML methods (but primarily used in an SVM).\n",
    "The idea behind it is that we can often express our model in terms of an inner product between feature vectors $(x^{(i)}, x^{(j)})$. This then allows us to take advantage of the useful properties of inner products to expand the feature space into a higher dimension, without having to actually calculate the data points in that dimension (which can be computationally inefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some mapping $\\phi$:\n",
    "\n",
    "$$\\phi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{t}, \\: t \\geq n $$\n",
    "$$x \\rightarrow \\phi(x) $$\n",
    "\n",
    "Essentially $\\phi$ is a mapping of our input vector into a vector of a higher dimensional (or different) space.\n",
    "Then we define what is known as the kernel function $K$ as:\n",
    "\n",
    "$$K: \\mathbb{R}^{2n} \\rightarrow \\mathbb{R} $$\n",
    "$$\\quad x, z \\in \\mathbb{R}^{n} \\rightarrow K(x, z) $$\n",
    "$$\\text{Where } \\: K(x, z) = \\phi(x)^T\\phi(z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially our Kernel function is simply the inner product of our vectors within the new space found by our function $\\phi$\n",
    "\n",
    "So all we have to do, is figure out how to compute $K(x, z)$ for some arbitrary $x, z$ and then in our model we are able to replace $(x^{(i)})^T(x^{(j)})$ with $K(x^{(i)}, x^{(j)})$ to learn within this transformed space $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What constitutes a valid Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we may pick a $\\phi(x)$ and then derive the kernel function from there. However in practice, we often go the other way around. This is because we often have some preconceived notion of which two data points are similar and dissimilar due to their outputs. Thus we want our kernel to output a very large number when these two data points are similar and a very small number when they are dissimilar. So in practice, we often pick the Kernel Function based on this notion. However we still need to make sure that the function we pick is a valid Kernel function.\n",
    "<br> By definition, a kernel function is valid if we can find a $\\phi$ such that our kernel can be expressed in it's defined form. From Mercer's Theorem, we can deduce that a kernel function is valid if and only if the following property is satisfied.\n",
    "<br> Given the Kernel Matrix (Gram Matrix) of any n data points:\n",
    "$$ K =\n",
    "\\begin{bmatrix} \n",
    "    K(x^{(1)}, x^{(1)}) & K(x^{(1)}, x^{(2)}) & \\dots & K(x^{(1)}, x^{(n)}) \\\\\n",
    "    K(x^{(2)}, x^{(2)}) & K(x^{(2)}, x^{(2)}) & \\dots & K(x^{(2)}, x^{(n)}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    K(x^{(n)}, x^{(1)}) & K(x^{(n)}, x^{(2)}) & \\dots & K(x^{(n)}, x^{(n)})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Kernel function is valid if and only if $K$ is a semi-definite matrix, which means K satisfies the following two properties:\n",
    "\n",
    "$$ K(x^{(i)}, x^{(j)}) = K(x^{(j)}, x^{(i)}) \\quad \\text{and} \\quad x^TKx \\geq 0, \\; \\forall x \\in \\mathbb{R}^n $$\n",
    "\n",
    "\n",
    "Again I will not prove this here, but it can be shown that this means we can derive a $\\phi$ to express $K(x, z)$ in it's defined form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will list some common kernels below (which kernel you should choose depends on the nature of your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Kernel:** \n",
    "$$ K(x, z) = x^Tz $$\n",
    "$$ \\phi(x) = x $$\n",
    "\n",
    "This is effectively the same as treating your data normally (does not change the feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Kernel (Radial Basis Function (RBF) Kernel)**:\n",
    "\n",
    "$$ K(x, z) = \\exp[-\\frac{||x - z||^2}{2\\sigma^2}] $$\n",
    "$$ \\phi(x) \\in \\mathbb{R}^\\infty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel, altough tricky to think about actually calculates the similarity of vectors in an infinite dimensional feature space consisting of every possible degree polynomial of the input vector's entries. This can be shown through Taylor Expansion of the Kernel function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel:**\n",
    "\n",
    "$$ K(x, z) = (x^Tz + c)^d $$\n",
    "$$ \\phi(x) \\in R^{{n + d}\\choose{d}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel calculates the similarity of vectors projected into a polynomial space, i.e. all possible combinations of polynomials up to degree $d$ of the input vector of degree $n$. The constant $c$ allows for a constant term to be included, which adds some bias to the mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Support Vector Machine (May be a bit math heavy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to seperate data points with the most optimal line in order to classify them correctly.\n",
    "In order to do so, we seek to correctly label our inputs given previous data. This means we seek a function\n",
    "$h_{\\theta}(x) \\in \\{-1, 1\\}$. For the purpose of SVMs we change our label space from $\\{0, 1\\}$ to $\\{-1, 1\\}$.\n",
    "In order for $h_{\\theta}$ to classify correctly we seek some linear decision boundary of which we can make our decision. So we seek a function $g(x)$, where:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\begin{cases}\n",
    "  1 & g(x) \\geq 0 \\\\\n",
    "  -1 & otherwise\n",
    "\\end{cases}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we may decide to use the kernel trick to attempt to change the feature space, if there is non-linearity in the data. Thus we will refer to our inputs now as $\\phi(x)$ rather than simply $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here, I will begin by specifying the SVM Hard Margin Classifier, and then explain the Soft Margin Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have completely linearly seperable data (or transformed data). Formally,\n",
    "$$\\exists \\; \\mathcal{H} : \\forall i \\in \\{1, ..., m\\}, \\; y^{(i)} \\cdot g(\\phi(x^{(i)})) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Functional Margin of a point as $y_{(i)}(\\theta^T\\phi(x^{(i)}) + b)$\n",
    "We can see that the functional margin is negative if $\\theta^T\\phi(x^{(i)}) + b$ results in a negative value and $y_{(i)}$ is actually a positive value. It is also negative if the inverse occurs. The functional margin is positive if and only if $y_{(i)}$ and $\\theta^T\\phi(x^{(i)})$ are both the same sign. This correlates to correctly or incorrectly classifying the output point. We then divide this functional margin by the norm of $\\theta$ so that it also accurately measures the corresponding distance of the margin to the decision boundary $\\mathcal{H} = \\{ \\phi(x) | \\theta^T\\phi(x) + b = 0 \\}$ We define the geometric margin as:\n",
    "\n",
    "$$ \\mathcal{J}^{(i)} = \\frac{y_{(i)}(\\theta^T\\phi(x^{(i)}) + b)}{||\\theta||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to find the minimum geometric margin of the entire set of points: $\\mathcal{J} = \\underset{i}{min} \\mathcal{J^{(i)}}$ and then maximize it. Essentially we are trying to maximize the smallest margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this problem simpler, let us scale $\\theta$ and $b$ such that the functional margin of all support vectors are 1. The support vectors are the closest points to the decision boundary $\\mathcal{H}$. By doing so we have:\n",
    "\n",
    "$$ y_{(i)}(\\theta^T\\phi(x^{(i)}) + b) = 1$$ \n",
    "\n",
    "For any support vectors. This also means that the geometric margin for any support vectors:\n",
    "\n",
    "$$ \\mathcal{J}^{(i)} = \\frac{1}{||\\theta||} $$\n",
    "\n",
    "And thus:\n",
    "\n",
    "$$ \\mathcal{J} = \\frac{1}{||\\theta||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our minimum geometric margin is a whole lot simpler to maximize, since we know the support vector's geometric margin. Thus we need to optimize:\n",
    "\n",
    "$$ \\underset{\\theta, b}{\\max}\\mathcal{J} = \\underset{\\theta, b}{\\max}\\frac{1}{||\\theta||} $$\n",
    "$$ = \\underset{\\theta, b}{\\min}\\mathcal{||\\theta||}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is subject to the constraint that the geometric margin of any point should be $\\geq \\mathcal{J}$ and this constraint is equivalent to the constraint that the functional margin of any point should be $\\geq 1$. Thus our optimization problem becomes:\n",
    "\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; ||\\theta|| $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; \\frac{1}{2}||\\theta||^2 $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; \\frac{1}{2}\\theta^T\\theta $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we use the method of the Langragian to simplify this problem.\n",
    "\n",
    "The Lagrangian is as follows:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta, b, \\alpha) = \\frac{1}{2}\\theta^T\\theta - \\sum_{i=1}^{m}\\alpha^{(i)}(y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the derivatives w.r.t the parameters:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\theta - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}) $$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting these derivaties equal to 0, we obtain:\n",
    "\n",
    "$$ \\theta = \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)})$$\n",
    "$$  0 = \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that our parameters $\\theta$ are actually linear combinations of our inputs and labels.\n",
    "\n",
    "Then we can substitute this back into our lagrangian. First let's calculate the original primal form with our new $\\theta$.\n",
    "\n",
    "$$\\frac{1}{2}\\theta^T\\theta = \\frac{1}{2}(\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))^T(\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))$$\n",
    "$$ =  \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}(\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))^T(\\alpha^{(j)}y^{(j)}\\phi(x^{(j)})) $$\n",
    "$$ = \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(i)})^T(\\phi(x^{(j)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the second part of the equation:\n",
    "\n",
    "$$ - \\sum_{i=1}^{m}\\alpha^{(i)}(y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) - 1) $$\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\theta^T\\phi(x^{(i)}) - b\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} $$\n",
    "\n",
    "Based on our constraint $\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} = 0$ we can remove this term to be left with:\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\theta^T\\phi(x^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can substitute our new theta back in:\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}(\\sum_{j=1}^{m}\\alpha^{(i)}y^{(i)}x^{(i)})^T\\phi(x^{(i)}) $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\sum_{j=1}^{m}\\alpha^{(j)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) $$\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) $$\n",
    "\n",
    "Finally, bringing these two components together we have\n",
    "\n",
    "$$\\mathcal{L}(\\alpha) = -\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) + \\sum_{i=1}^{m}\\alpha^{(i)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our optimization problem in dual form becomes:\n",
    "\n",
    "$$ \\underset{\\alpha}{\\max}[\\sum_{i=1}^{m}\\alpha^{(i)} - \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)})] $$\n",
    "$$ \\text{s.t. } \\; \\sum_{i=1}^{m} = 0, \\alpha_{i} \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can now feed into an optimization alogorithm to solve. Keep in mind this form allows us to re-write the equation in terms of the kernel function described above, which allows us to take advantage of the kernel trick if we'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin SVM\n",
    "\n",
    "Now that I have established the objective of the hard margin SVM, I will introduce the soft margin SVM which eases the constraints slightly to allow for some misclassifications in the data. This is good if your data (or transformed data) is almost linearly seperable, with the exception of a small amount of outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ease the constraint, we recall the functional margin of the support vectors as: $y_{(i)}(\\theta^T\\phi(x^{(i)}) + b)$ = 1$. Previously in our optimization problem, we had the constraint that all functional margins should be at least 1. However we will now introduce a slack variable $\\xi^{(i)}$ to ease that constraint slightly. So our constraint becomes $$y_{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 - \\xi^{(i)}, \\xi^{(i)} \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to penalize for when this $\\xi^{(i)} \\geq 0$. If it is greater than 0 but less than one, it means the data point is within the margin of the support vectors but still on the correct side of the hyperplane, and if it is greater than 1 it means that it is on the opposite side of the hyperplane. So, our objective function should total up all of these penalties, of course multiplied by some constant hyperparameter. Thus our objective function becomes:\n",
    "\n",
    "$$\\underset{\\theta, b}{\\min}[\\frac{1}{2}\\theta^T\\theta + C\\sum_{i=1}^{m}\\xi^{(i)}]$$\n",
    "\n",
    "Where $C \\geq 0$ is our hyperparameter that we can control to adjust the level of softness we allow. Note that if $C = 0$ then this problem simplifies to the hard margin optimization problem above. Following a similar derivation process as we did above, we obtain the following dual form optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\underset{\\alpha}{\\max} \\; \\sum_{i=1}^{m}\\alpha^{(i)} - \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(i)})^T\\phi(x^{(j)})$$\n",
    "$$ \\text{s.t.} \\; 0 \\leq \\alpha^{(i)} \\leq C$$\n",
    "$$ \\quad \\sum_{i=1}^{m} \\alpha^{(i)}y^{(i)} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the Dual Form Soft Margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to solve it with some iterative optimization algorithm, we require the derivative of our Dual form, with respect to the lagrange multipliers. Now keep in mind we usually treat these as minimization problems so we will negate our Lagrangian function.\n",
    "\n",
    "Let $$J(\\alpha) = -\\mathcal{L}(\\alpha)$$\n",
    "\n",
    "So we have,\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\alpha^{(k)}} = \\sum_{i = 1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(k)})^T(x^{(i)}) - 1$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

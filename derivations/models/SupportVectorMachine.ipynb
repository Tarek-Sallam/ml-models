{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to the Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of Support Vector Machine?\n",
    "\n",
    "A support vector machine (SVM) is natively a binary classification supervised machine learning method. On it's own, a support vector machine is not particularly powerful, however when paired with what is known as the kernel trick, it becomes a very powerful tool for classifying data that is not obviously linearly seperable.\n",
    "\n",
    "Due to the nature of this pairing, I will first go over the kernel trick, and then after the SVM and how (and why) we can apply the kernel trick to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is a very useful tool that we can use in a variety of ML methods (but primarily used in an SVM).\n",
    "The idea behind it is that we can often express our model in terms of an inner product between feature vectors $(x^{(i)}, x^{(j)})$. This then allows us to take advantage of the useful properties of inner products to expand the feature space into a higher dimension, without having to actually calculate the data points in that dimension (which can be computationally inefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some mapping $\\phi$:\n",
    "\n",
    "$$\\phi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{t}, \\: t \\geq n $$\n",
    "$$x \\rightarrow \\phi(x) $$\n",
    "\n",
    "Essentially $\\phi$ is a mapping of our input vector into a vector of a higher dimensional (or different) space.\n",
    "Then we define what is known as the kernel function $K$ as:\n",
    "\n",
    "$$K: \\mathbb{R}^{2n} \\rightarrow \\mathbb{R} $$\n",
    "$$\\quad x, z \\in \\mathbb{R}^{n} \\rightarrow K(x, z) $$\n",
    "$$\\text{Where } \\: K(x, z) = \\phi(x)^T\\phi(z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially our Kernel function is simply the inner product of our vectors within the new space found by our function $\\phi$\n",
    "\n",
    "So all we have to do, is figure out how to compute $K(x, z)$ for some arbitrary $x, z$ and then in our model we are able to replace $(x^{(i)})^T(x^{(j)})$ with $K(x^{(i)}, x^{(j)})$ to learn within this transformed space $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What constitutes a valid Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we may pick a $\\phi(x)$ and then derive the kernel function from there. However in practice, we often go the other way around. This is because we often have some preconceived notion of which two data points are similar and dissimilar due to their outputs. Thus we want our kernel to output a very large number when these two data points are similar and a very small number when they are dissimilar. So in practice, we often pick the Kernel Function based on this notion. However we still need to make sure that the function we pick is a valid Kernel function.\n",
    "<br> By definition, a kernel function is valid if we can find a $\\phi$ such that our kernel can be expressed in it's defined form. From Mercer's Theorem, we can deduce that a kernel function is valid if and only if the following property is satisfied.\n",
    "<br> Given the Kernel Matrix (Gram Matrix) of any n data points:\n",
    "$$ K =\n",
    "\\begin{bmatrix} \n",
    "    K(x^{(1)}, x^{(1)}) & K(x^{(1)}, x^{(2)}) & \\dots & K(x^{(1)}, x^{(n)}) \\\\\n",
    "    K(x^{(2)}, x^{(2)}) & K(x^{(2)}, x^{(2)}) & \\dots & K(x^{(2)}, x^{(n)}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    K(x^{(n)}, x^{(1)}) & K(x^{(n)}, x^{(2)}) & \\dots & K(x^{(n)}, x^{(n)})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Kernel function is valid if and only if $K$ is a semi-definite matrix, which means K satisfies the following two properties:\n",
    "\n",
    "$$ K(x^{(i)}, x^{(j)}) = K(x^{(j)}, x^{(i)}) \\quad \\text{and} \\quad x^TKx \\geq 0, \\; \\forall x \\in \\mathbb{R}^n $$\n",
    "\n",
    "\n",
    "Again I will not prove this here, but it can be shown that this means we can derive a $\\phi$ to express $K(x, z)$ in it's defined form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will list some common kernels below (which kernel you should choose depends on the nature of your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Kernel:** \n",
    "$$ K(x, z) = x^Tz $$\n",
    "$$ \\phi(x) = x $$\n",
    "\n",
    "This is effectively the same as treating your data normally (does not change the feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Kernel (Radial Basis Function (RBF) Kernel)**:\n",
    "\n",
    "$$ K(x, z) = \\exp[-\\frac{||x - z||^2}{2\\sigma^2}] $$\n",
    "$$ \\phi(x) \\in \\mathbb{R}^\\infty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel, altough tricky to think about actually calculates the similarity of vectors in an infinite dimensional feature space consisting of every possible degree polynomial of the input vector's entries. This can be shown through Taylor Expansion of the Kernel function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel:**\n",
    "\n",
    "$$ K(x, z) = (x^Tz + c)^d $$\n",
    "$$ \\phi(x) \\in R^{{n + d}\\choose{d}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel calculates the similarity of vectors projected into a polynomial space, i.e. all possible combinations of polynomials up to degree $d$ of the input vector of degree $n$. The constant $c$ allows for a constant term to be included, which adds some bias to the mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Support Vector Machine (May be a bit math heavy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to seperate data points with the most optimal line in order to classify them correctly.\n",
    "In order to do so, we seek to correctly label our inputs given previous data. This means we seek a function\n",
    "$h_{\\theta}(x) \\in \\{-1, 1\\}$. For the purpose of SVMs we change our label space from $\\{0, 1\\}$ to $\\{-1, 1\\}$.\n",
    "In order for $h_{\\theta}$ to classify correctly we seek some linear decision boundary of which we can make our decision. So we seek a function $g(x)$, where:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\begin{cases}\n",
    "  1 & g(x) \\geq 0 \\\\\n",
    "  -1 & otherwise\n",
    "\\end{cases}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we may decide to use the kernel trick to attempt to change the feature space, if there is non-linearity in the data. Thus we will refer to our inputs now as $\\phi(x)$ rather than simply $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this $g(\\phi(x))$ represents a hyperplane when $g(\\phi(x)) = 0$. The decision function is simply $\\theta^Tx + b$, however we will show that this equals\n",
    "$$\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}K(x^{(i)}, x) + b$$\n",
    "\n",
    "Which will make sense after we go over the dual form of SVM and kernel functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here, I will begin by specifying the SVM Hard Margin Classifier, and then explain the Soft Margin Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have completely linearly seperable data (or transformed data). Formally,\n",
    "$$\\exists \\; \\mathcal{H} : \\forall i \\in \\{1, ..., m\\}, \\; y^{(i)} \\cdot g(\\phi(x^{(i)})) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Functional Margin of a point as $y_{(i)}(\\theta^T\\phi(x^{(i)}) + b)$\n",
    "We can see that the functional margin is negative if $\\theta^T\\phi(x^{(i)}) + b$ results in a negative value and $y_{(i)}$ is actually a positive value. It is also negative if the inverse occurs. The functional margin is positive if and only if $y_{(i)}$ and $\\theta^T\\phi(x^{(i)})$ are both the same sign. This correlates to correctly or incorrectly classifying the output point. We then divide this functional margin by the norm of $\\theta$ so that it also accurately measures the corresponding distance of the margin to the decision boundary $\\mathcal{H} = \\{ \\phi(x) | \\theta^T\\phi(x) + b = 0 \\}$ We define the geometric margin as:\n",
    "\n",
    "$$ \\mathcal{J}^{(i)} = \\frac{y_{(i)}(\\theta^T\\phi(x^{(i)}) + b)}{||\\theta||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to find the minimum geometric margin of the entire set of points: $\\mathcal{J} = \\underset{i}{min} \\mathcal{J^{(i)}}$ and then maximize it. Essentially we are trying to maximize the smallest margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this problem simpler, let us scale $\\theta$ and $b$ such that the functional margin of all support vectors are 1. The support vectors are the closest points to the decision boundary $\\mathcal{H}$. By doing so we have:\n",
    "\n",
    "$$ y_{(i)}(\\theta^T\\phi(x^{(i)}) + b) = 1$$ \n",
    "\n",
    "For any support vectors. This also means that the geometric margin for any support vectors:\n",
    "\n",
    "$$ \\mathcal{J}^{(i)} = \\frac{1}{||\\theta||} $$\n",
    "\n",
    "And thus:\n",
    "\n",
    "$$ \\mathcal{J} = \\frac{1}{||\\theta||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our minimum geometric margin is a whole lot simpler to maximize, since we know the support vector's geometric margin. Thus we need to optimize:\n",
    "\n",
    "$$ \\underset{\\theta, b}{\\max}\\mathcal{J} = \\underset{\\theta, b}{\\max}\\frac{1}{||\\theta||} $$\n",
    "$$ = \\underset{\\theta, b}{\\min}\\mathcal{||\\theta||}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is subject to the constraint that the geometric margin of any point should be $\\geq \\mathcal{J}$ and this constraint is equivalent to the constraint that the functional margin of any point should be $\\geq 1$. Thus our optimization problem becomes:\n",
    "\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; ||\\theta|| $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; \\frac{1}{2}||\\theta||^2 $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n",
    "$$ = \\underset{\\theta, b}{\\argmin} \\; \\frac{1}{2}\\theta^T\\theta $$\n",
    "$$ \\text{s.t.} \\; y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) \\geq 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we use the method of the Langragian to simplify this problem.\n",
    "\n",
    "The Lagrangian is as follows:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta, b, \\alpha) = \\frac{1}{2}\\theta^T\\theta - \\sum_{i=1}^{m}\\alpha^{(i)}(y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the derivatives w.r.t the parameters:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\theta - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}) $$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting these derivaties equal to 0, we obtain:\n",
    "\n",
    "$$ \\theta = \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)})$$\n",
    "$$  0 = \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that our parameters $\\theta$ are actually linear combinations of our inputs and labels.\n",
    "\n",
    "Then we can substitute this back into our lagrangian. First let's calculate the original primal form with our new $\\theta$.\n",
    "\n",
    "$$\\frac{1}{2}\\theta^T\\theta = \\frac{1}{2}(\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))^T(\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))$$\n",
    "$$ =  \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}(\\alpha^{(i)}y^{(i)}\\phi(x^{(i)}))^T(\\alpha^{(j)}y^{(j)}\\phi(x^{(j)})) $$\n",
    "$$ = \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(i)})^T(\\phi(x^{(j)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the second part of the equation:\n",
    "\n",
    "$$ - \\sum_{i=1}^{m}\\alpha^{(i)}(y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) - 1) $$\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}(\\theta^T\\phi(x^{(i)}) + b) $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\theta^T\\phi(x^{(i)}) - b\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} $$\n",
    "\n",
    "Based on our constraint $\\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} = 0$ we can remove this term to be left with:\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\theta^T\\phi(x^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can substitute our new theta back in:\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}(\\sum_{j=1}^{m}\\alpha^{(i)}y^{(i)}x^{(i)})^T\\phi(x^{(i)}) $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)}\\sum_{j=1}^{m}\\alpha^{(j)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) $$\n",
    "$$ = \\sum_{i=1}^{m}\\alpha^{(i)} - \\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) $$\n",
    "\n",
    "Finally, bringing these two components together we have\n",
    "\n",
    "$$\\mathcal{L}(\\alpha) = -\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)}) + \\sum_{i=1}^{m}\\alpha^{(i)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our optimization problem in dual form becomes:\n",
    "\n",
    "$$ \\underset{\\alpha}{\\max}[\\sum_{i=1}^{m}\\alpha^{(i)} - \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)})] $$\n",
    "$$ \\text{s.t. } \\; \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} = 0, \\alpha^{(i)} \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can now feed into an optimization alogorithm to solve. Keep in mind this form allows us to re-write the equation in terms of the kernel function described above, which allows us to take advantage of the kernel trick if we'd like. In convex optimization, these constraints correspond to the dual form lagrangian, and the KKT conditions are:\n",
    "<br>\n",
    "<br> Stationary Condition: $\\theta = \\sum_{i = 1}^{m}\\alpha^{(i)}y^{(i)}\\phi({x^{(i)}}), \\sum_{i = 1}^{m}\\alpha^{(i)}y^{(i)} = 0 $\n",
    "<br>\n",
    "<br> Primal Feasability: $y^{(i)}\\phi({x^{(i)}}) \\geq 1$\n",
    "<br>\n",
    "<br> Dual Feasability: $\\alpha^{(i)} \\geq 0$\n",
    "<br>\n",
    "<br> Complementary Slackness: $\\alpha^{(i)}(y^{(i)}\\phi({x^{(i)}}) - 1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last condition allows us to take advantage of the fact that only the data points that lie on the margin i.e have $y^{(i)}\\phi(x^{(i)}) = 1$, will have a corresponding $\\alpha^{(i)}$ and all the points that do not lie on the margin, will have an $\\alpha^{(i)} = 0$. So we can actually determine the class of new data points solely on the data points with lagrangian multipliers non-zero. These points are known as the support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the above solution will find some hyperplane that perfectly seperates the two classes. This may not be possible and thus a solution may not be found with the Hard Margin SVM. So we must extend the idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add some variables $\\xi^{(i)}$ that introduce some slack to each margin, then we get the margins as \n",
    "$y^{(i)}\\phi(x^{(i)}) \\geq 1 - \\xi^{(i)}$. This means if $\\xi^{(i)} = 0$ we have correctly classified the point and it lies on or outside the margin. Since a point lies in the margin if the functional margin is $= 1$. However if we have $0 < \\xi^{(i)} < 1$ we have that the point is classified correctly, but it lies inside the margin. Lastly if we have $\\xi^{(i)} \\geq 1$ Then the point is classified incorrectly, lies on the hyperplane or on the opposite side of the hyperplane. Of course we want to allow these slack variables to take on values $\\geq 0$ but how much we wish to allow depends on a hyperparameter $C \\geq 0$. Note that if $C = 0$ we obtain the hard margin SVM. So we now wish to maximize the margin, while minimizing the sum of all these slack variables. Thus our primal form optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\underset{\\theta, b, \\xi}{\\min} \\; \\frac{1}{2} \\theta^T\\theta + C\\sum_{i=1}^{m}\\xi^{(i)}\n",
    "    \\\\&\\text{s.t.} \\; y^{(i)}\\phi(x^{(i)}) \\geq 1 - \\xi^{(i)}, \\; \\xi^{(i)} \\geq 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual form can be derived similarly to above, and results in the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    &\\underset{\\alpha}{\\max}[\\sum_{i=1}^{m}\\alpha^{(i)} - \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}y^{(i)}y^{(j)}\\phi(x^{(j)})^T\\phi(x^{(i)})]\n",
    "    \\\\&\\text{s.t.} \\; \\sum_{i=1}^{m}\\alpha^{(i)}y^{(i)} = 0, \\; 0 < \\alpha^{(i)} \\leq C\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

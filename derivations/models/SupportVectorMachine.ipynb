{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivations related to the Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of Support Vector Machine?\n",
    "\n",
    "A support vector machine (SVM) is natively a binary classification supervised machine learning method. On it's own, a support vector machine is not particularly powerful, however when paired with what is known as the kernel trick, it becomes a very powerful tool for classifying data that is not obviously linearly seperable.\n",
    "\n",
    "Due to the nature of this pairing, I will first go over the kernel trick, and then after the SVM and how (and why) we can apply the kernel trick to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is a very useful tool that we can use in a variety of ML methods (but primarily used in an SVM).\n",
    "The idea behind it is that we can often express our model in terms of an inner product between feature vectors $(x^{(i)}, x^{(j)})$. This then allows us to take advantage of the useful properties of inner products to expand the feature space into a higher dimension, without having to actually calculate the data points in that dimension (which can be computationally inefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some mapping $\\phi$:\n",
    "\n",
    "$$\\phi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{t}, \\: t \\geq n $$\n",
    "$$x \\rightarrow \\phi(x) $$\n",
    "\n",
    "Essentially $\\phi$ is a mapping of our input vector into a vector of a higher dimensional (or different) space.\n",
    "Then we define what is known as the kernel function $K$ as:\n",
    "\n",
    "$$K: \\mathbb{R}^{2n} \\rightarrow \\mathbb{R} $$\n",
    "$$\\quad x, z \\in \\mathbb{R}^{n} \\rightarrow K(x, z) $$\n",
    "$$\\text{Where } \\: K(x, z) = \\phi(x)^T\\phi(z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially our Kernel function is simply the inner product of our vectors within the new space found by our function $\\phi$\n",
    "\n",
    "So all we have to do, is figure out how to compute $K(x, z)$ for some arbitrary $x, z$ and then in our model we are able to replace $(x^{(i)})^T(x^{(j)})$ with $K(x^{(i)}, x^{(j)})$ to learn within this transformed space $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What constitutes a valid Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we may pick a $\\phi(x)$ and then derive the kernel function from there. However in practice, we often go the other way around. This is because we often have some preconceived notion of which two data points are similar and dissimilar due to their outputs. Thus we want our kernel to output a very large number when these two data points are similar and a very small number when they are dissimilar. So in practice, we often pick the Kernel Function based on this notion. However we still need to make sure that the function we pick is a valid Kernel function.\n",
    "<br> By definition, a kernel function is valid if we can find a $\\phi$ such that our kernel can be expressed in it's defined form. From Mercer's Theorem, we can deduce that a kernel function is valid if and only if the following property is satisfied.\n",
    "<br> Given the Kernel Matrix (Gram Matrix) of any n data points:\n",
    "$$ K =\n",
    "\\begin{bmatrix} \n",
    "    K(x^{(1)}, x^{(1)}) & K(x^{(1)}, x^{(2)}) & \\dots & K(x^{(1)}, x^{(n)}) \\\\\n",
    "    K(x^{(2)}, x^{(2)}) & K(x^{(2)}, x^{(2)}) & \\dots & K(x^{(2)}, x^{(n)}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    K(x^{(n)}, x^{(1)}) & K(x^{(n)}, x^{(2)}) & \\dots & K(x^{(n)}, x^{(n)})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Kernel function is valid if and only if $K$ is a semi-definite matrix, which means K satisfies the following two properties:\n",
    "\n",
    "$$ K(x^{(i)}, x^{(j)}) = K(x^{(j)}, x^{(i)}) \\quad \\text{and} \\quad x^TKx \\geq 0, \\; \\forall x \\in \\mathbb{R}^n $$\n",
    "\n",
    "\n",
    "Again I will not prove this here, but it can be shown that this means we can derive a $\\phi$ to express $K(x, z)$ in it's defined form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will list some common kernels below (which kernel you should choose depends on the nature of your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Kernel:** \n",
    "$$ K(x, z) = x^Tz $$\n",
    "$$ \\phi(x) = x $$\n",
    "\n",
    "This is effectively the same as treating your data normally (does not change the feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Kernel (Radial Basis Function (RBF) Kernel)**:\n",
    "\n",
    "$$ K(x, z) = \\exp[-\\frac{||x - z||^2}{2\\sigma^2}] $$\n",
    "$$ \\phi(x) \\in \\mathbb{R}^\\infty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel, altough tricky to think about actually calculates the similarity of vectors in an infinite dimensional feature space consisting of every possible degree polynomial of the input vector's entries. This can be shown through Taylor Expansion of the Kernel function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel:**\n",
    "\n",
    "$$ K(x, z) = (x^Tz + c)^d $$\n",
    "$$ \\phi(x) \\in R^{{n + d}\\choose{d}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kernel calculates the similarity of vectors projected into a polynomial space, i.e. all possible combinations of polynomials up to degree $d$ of the input vector of degree $n$. The constant $c$ allows for a constant term to be included, which adds some bias to the mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to seperate data points with the most optimal line in order to classify them correctly.\n",
    "In order to do so, we seek to correctly label our inputs given previous data. This means we seek a function\n",
    "$h_{\\theta}(x) \\in \\{-1, 1\\}$. For the purpose of SVMs we change our label space from $\\{0, 1\\}$ to $\\{-1, 1\\}$.\n",
    "In order for $h_{\\theta}$ to classify correctly we seek some linear decision boundary of which we can make our decision. So we seek a function $g(x)$, where:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\begin{cases}\n",
    "  1 & g(x) \\geq 0 \\\\\n",
    "  -1 & otherwise\n",
    "\\end{cases}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we seek a linear decision boundary (similar to logistic regression) such that we can classify points that are above and below that boundary are classified as seperate classes. Let this function g(x) be a linear function:\n",
    "\n",
    "$$ g(x) = \\theta^Tx + b $$\n",
    "\n",
    "Since we guess class $1$ for positive outputs and class $-1$ for negative outputs, we can clearly see that the \"decision boundary\" is determined by the hyperplane $$\\mathcal{H} = \\{x \\: | \\: \\theta^Tx + b = 0\\} $$\n",
    "\n",
    "Which is the set of points that satisfy our equation when set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we may decide to use the kernel trick to attempt to change the feature space, if there is non-linearity in the data. Thus we will refer to our inputs now as $\\phi(x)$ rather than simply $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here, I will begin by specifying the SVM Hard Margin Classifier, and then explain the Soft Margin Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have completely linearly seperable data (or transformed data). Formally,\n",
    "$$\\exists \\; \\mathcal{H} : \\forall i \\in \\{1, ..., m\\}, \\; y^{(i)} \\cdot g(\\phi(x^{(i)})) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define parallel hyperplanes that run through the data points closest to either side of our hyperplane $\\mathcal{H}$: $$\\mathcal{H}_{\\alpha} = \\{x \\: | \\: \\theta^Tx + \\alpha = 0\\} $$\n",
    "$$\\mathcal{H}_{\\beta} = \\{x \\: | \\: \\theta^Tx + \\beta = 0\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data points that our closest to our hyperplane $\\mathcal{H}$ are referred to as 'support vectors'. Without allowing any misclassifications, (changing our hyperplane so that it does not satisfy the condition above), we believe that the best hyperplane is the one that maximizes the distance between our two parallel hyerplanes $\\mathcal{H}_\\alpha$ and $\\mathcal{H}_\\beta$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a support vector on both of our planes:\n",
    "<br>$x_{\\beta} \\in \\mathcal{H}_{\\beta}$, and \n",
    "<br>$x_{\\alpha} \\in \\mathcal{H}_{\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can then take the vector between these two points as, <br>$(x_{\\beta}-x_{\\alpha})$ and take the magnitude of the projection of it onto the normal vector of our central hyperplane $\\mathcal{H}$. This then gives us the total distance between our two hyperplanes $\\mathcal{H}_{\\alpha}$ and $\\mathcal{H}_{\\beta}$. This distance is know as the margin of the hyperplane $\\mathcal{H}$. Given the definition of our hyperplane, we have the normal vector of the hyperplane as $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The margin is as follows:\n",
    "\n",
    "$$\\frac{|\\theta^T(x_{\\beta} - x_{\\alpha})|}{||\\theta||}$$\n",
    "$$=\\frac{|\\theta^Tx_{\\beta} - \\theta^Tx_\\alpha|}{||\\theta||}$$\n",
    "$$=\\frac{|-\\beta - (- \\alpha)|}{||\\theta||}$$\n",
    "$$=\\frac{|\\alpha - \\beta|}{||\\theta||}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our Hyperplane must create the maximum margin, which is equivalent to maximizing the distance from the support vectors to the line, we can deduce that our Hyperplane $\\mathcal{H}$ is right in the middle of our two hyperplanes that run through our support vectors. Thus we can state that $\\beta = b + c$ and $\\alpha = b - c$ for some constant $c$. Also, due to the inherent nature of hyperplanes, we are allowed to scale $b$ so long as we scale $\\theta$ so that the hyperplane equation remains the same. This allows us to assume WLOG that our $\\beta = 1 + b$ and our $\\alpha = b - 1$, so long as we scale $\\theta, b$ appropriately. Thus the margin becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{|(b - 1) - (1 + b)|}{||\\theta||} $$\n",
    "$$ = \\frac{2}{||\\theta||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we are trying to maximize this margin, so we need to find $\\theta$ and $b$ that maximizes this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\theta, b}{\\mathrm{argmax}} \\frac{2}{||\\theta||}$$\n",
    "$$= \\underset{\\theta, b}{\\mathrm{argmin}} \\frac{||\\theta||}{2}$$\n",
    "$$= \\underset{\\theta, b}{\\mathrm{argmin}} \\frac{||\\theta||^2}{2}$$\n",
    "$$= \\underset{\\theta, b}{\\mathrm{argmin}} \\frac{\\theta^T\\theta}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't make any sense without an additional constraint. Yes we are trying to maximize the margin but we still want a valid hyperplane that satisfies the requirement to seperate the data into each class. For all points $x_\\beta$ where $g(x_\\beta) = \\theta^Tx_\\beta + b > 0$, it is also true that $\\theta^Tx_\\beta + \\beta > 0$ since our hyperplane $\\mathcal{H}_{\\beta}$ is defined on the support vector(s). Since $\\beta = b + 1$, we can deduce the following:\n",
    "\n",
    "$$\\forall x_\\beta \\in x : g(x_\\beta) > 0,$$\n",
    "$$\\theta^Tx_\\beta + b \\geq 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we can deduce:\n",
    "$$\\forall x_\\beta \\in x : g(x_\\beta) < 0,$$\n",
    "$$\\theta^Tx_\\beta + b \\leq -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we know that the points that are on one side will be classified as less than -1 and the others will be classified as greater than 1. If we get a point classified as less than -1, than the real output should be -1, and likewise for the positives. Thus if we multiply the real output with our predicted output we should get the constraint:\n",
    "\n",
    "$$ y^{(i)}(\\theta^Tx^{(i)} + b) \\geq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence our optimization problem becomes:\n",
    "\n",
    "$$\\underset{\\theta, b}{\\mathrm{min}} \\frac{\\theta^T\\theta}{2}$$\n",
    "$$ \\textrm{s.t.} \\quad y^{(i)}(\\theta^Tx^{(i)} + b) \\geq 1 $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
